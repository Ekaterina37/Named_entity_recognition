{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uFXI7NRHn1Cc"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "path = ''\n",
        "\n",
        "with open(os.path.join(path,'train.json'), 'r') as f:\n",
        "     train = json.loads(f.read())\n",
        "\n",
        "with open(os.path.join(path,'test.json'), 'r') as f:\n",
        "     test = json.loads(f.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvvRSlhb6sAR",
        "outputId": "af481a3e-f337-45a3-deee-a7b68f35ee56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Romania', 'state', 'budget', 'soars', 'in', 'June', '.', 'BUCHAREST', '1996-08-28', 'Romania', \"'s\", 'state', 'budget', 'deficit', 'jumped', 'sharply', 'in', 'June', 'to', '1,242.9', 'billion', 'lei', 'for', 'the', 'January-June', 'period', 'from', '596.5', 'billion', 'lei', 'in', 'January-May', ',', 'official', 'data', 'showed', 'on', 'Wednesday', '.', 'Six-month', 'expenditures', 'stood', 'at', '9.50', 'trillion', 'lei', ',', 'up', 'from', '7.56', 'trillion', 'lei', 'at', 'end-May', ',', 'with', 'education', 'and', 'health', 'spending', 'accounting', 'for', '31.6', 'percent', 'of', 'state', 'expenses', 'and', 'economic', 'subsidies', 'and', 'support', 'taking', 'some', '26', 'percent', '.', 'January-June', 'revenues', 'went', 'up', 'to', '8.26', 'trillion', 'lei', 'from', '6.96', 'trillion', 'lei', 'in', 'the', 'first', 'five', 'months', 'this', 'year', '.', 'Romania', \"'s\", 'government', 'is', 'expected', 'to', 'revise', 'the', '1996', 'budget', 'on', 'Wednesday', 'to', 'bring', 'it', 'into', 'line', 'with', 'higher', 'inflation', ',', 'new', 'wage', 'and', 'pension', 'indexations', 'and', 'costs', 'of', 'energy', 'imports', 'that', 'have', 'pushed', 'up', 'the', 'state', 'deficit', '.', 'Under', 'the', 'revised', 'version', 'state', 'spending', 'is', 'expected', 'to', 'rise', 'by', 'some', '566', 'billion', 'lei', '.', 'No', 'new', 'deficit', 'forecast', 'has', 'been', 'issued', 'so', 'far', '.', 'In', 'July', 'the', 'government', 'gave', 'a', '6.0-percent', 'wage', 'and', 'pension', 'indexation', 'to', 'cover', 'energy', ',', 'fuel', 'and', 'bread', 'price', 'increases', ',', 'which', 'quickened', 'inflation', 'to', '7.5', 'percent', 'last', 'month', '.', 'In', 'the', 'original', 'state', 'budget', ',', 'approved', 'in', 'March', ',', 'revenues', 'were', 'envisaged', 'at', 'around', '16.98', 'trillion', 'lei', 'and', 'expenditures', '20.17', 'trillion', 'lei', 'for', '1996', '.', 'The', 'state', 'budget', 'deficit', 'was', 'originally', 'forecast', 'to', 'be', '3.19', 'trillion', 'lei', 'for', 'the', 'whole', 'year', '.', 'On', 'Wednesday', ',', 'the', 'leu', \"'s\", 'official', 'rate', 'was', '3,161', 'to', 'the', 'dollar', '.', '--', 'Bucharest', 'Newsroom', '40-1', '3120264']\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253]\n",
            "['NNP', 'NN', 'NN', 'NNS', 'IN', 'NNP', '.', 'RB', 'CD', 'NNP', 'POS', 'NN', 'NN', 'NN', 'VBD', 'RB', 'IN', 'NNP', 'TO', 'CD', 'CD', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'CD', 'CD', 'VBN', 'IN', 'NNP', ',', 'JJ', 'NNS', 'VBD', 'IN', 'NNP', '.', 'JJ', 'NNS', 'VBD', 'IN', 'CD', 'CD', 'NNS', ',', 'RB', 'IN', 'CD', 'CD', 'VBD', 'IN', 'NN', ',', 'IN', 'NN', 'CC', 'NN', 'NN', 'NN', 'IN', 'CD', 'NN', 'IN', 'NN', 'NNS', 'CC', 'JJ', 'NNS', 'CC', 'VB', 'VBG', 'DT', 'CD', 'NN', '.', 'JJ', 'NNS', 'VBD', 'IN', 'TO', 'CD', 'CD', 'NN', 'IN', 'CD', 'CD', 'VBN', 'IN', 'DT', 'JJ', 'CD', 'NNS', 'DT', 'NN', '.', 'NNP', 'POS', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'DT', 'CD', 'NN', 'IN', 'NNP', 'TO', 'VB', 'PRP', 'IN', 'NN', 'IN', 'JJR', 'NN', ',', 'JJ', 'NN', 'CC', 'NN', 'NNS', 'CC', 'NNS', 'IN', 'NN', 'NNS', 'WDT', 'VBP', 'VBN', 'RP', 'DT', 'NN', 'NN', '.', 'IN', 'DT', 'VBN', 'NN', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'IN', 'DT', 'CD', 'CD', 'NNS', '.', 'DT', 'JJ', 'NN', 'NN', 'VBZ', 'VBN', 'VBN', 'RB', 'RB', '.', 'IN', 'NNP', 'DT', 'NN', 'VBD', 'DT', 'JJ', 'NN', 'CC', 'NN', 'NN', 'TO', 'VB', 'NN', ',', 'NN', 'CC', 'NN', 'NN', 'NNS', ',', 'WDT', 'VBN', 'NN', 'TO', 'CD', 'NN', 'JJ', 'NN', '.', 'IN', 'DT', 'JJ', 'NN', 'NN', ',', 'VBN', 'IN', 'NNP', ',', 'NNS', 'VBD', 'VBN', 'IN', 'IN', 'CD', 'CD', 'NNS', 'CC', 'NNS', 'CD', 'CD', 'NN', 'IN', 'CD', '.', 'DT', 'NN', 'NN', 'NN', 'VBD', 'RB', 'VBN', 'TO', 'VB', 'CD', 'CD', 'NN', 'IN', 'DT', 'JJ', 'NN', '.', 'IN', 'NNP', ',', 'DT', 'NN', 'POS', 'JJ', 'NN', 'VBD', 'CD', 'TO', 'DT', 'NN', '.', ':', 'NNP', 'NNP', 'NNP', 'CD']\n",
            "['LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'LOC', 'O', 'LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ORG', 'ORG', 'O', 'O']\n"
          ]
        }
      ],
      "source": [
        "print(train['text'][0])\n",
        "print(train['index'][0])\n",
        "print(train['POS'][0])\n",
        "print(train['NER'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DiF6W0UkIWk_"
      },
      "outputs": [],
      "source": [
        "def format_output_labels(token_labels, token_indices):\n",
        "\n",
        "    label_dict = {\"LOC\":[], \"MISC\":[], \"ORG\":[], \"PER\":[]}\n",
        "    prev_label = token_labels[0]\n",
        "    start = token_indices[0]\n",
        "    for idx, label in enumerate(token_labels):\n",
        "      if prev_label != label:\n",
        "        end = token_indices[idx-1]\n",
        "        if prev_label != \"O\":\n",
        "            label_dict[prev_label].append((start, end))\n",
        "        start = token_indices[idx]\n",
        "      prev_label = label\n",
        "      if idx == len(token_labels) - 1:\n",
        "        if prev_label != \"O\":\n",
        "            label_dict[prev_label].append((start, token_indices[idx]))\n",
        "    return label_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zCPOGrXrIZIF"
      },
      "outputs": [],
      "source": [
        "# Code for mean F1\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def mean_f1(y_pred_dict, y_true_dict):\n",
        "\n",
        "    F1_lst = []\n",
        "    for key in y_true_dict:\n",
        "        TP, FN, FP = 0, 0, 0\n",
        "        num_correct, num_true = 0, 0\n",
        "        preds = y_pred_dict[key]\n",
        "        trues = y_true_dict[key]\n",
        "        for true in trues:\n",
        "            num_true += 1\n",
        "            if true in preds:\n",
        "                num_correct += 1\n",
        "            else:\n",
        "                continue\n",
        "        num_pred = len(preds)\n",
        "        if num_true != 0:\n",
        "            if num_pred != 0 and num_correct != 0:\n",
        "                R = num_correct / num_true\n",
        "                P = num_correct / num_pred\n",
        "                F1 = 2*P*R / (P + R)\n",
        "            else:\n",
        "                F1 = 0 \n",
        "        else:\n",
        "            continue\n",
        "        F1_lst.append(F1)\n",
        "    return np.mean(F1_lst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1gh7A0sIbVj",
        "outputId": "280062a6-a943-4e49-8394-0c9a2c065c8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "y_pred_dict is : {'LOC': [(18, 18), (28, 28)], 'MISC': [(23, 23)], 'ORG': [(13, 13)], 'PER': [(15, 16)]}\n",
            "y_true_dict is : {'LOC': [(18, 18), (28, 28)], 'MISC': [(23, 24)], 'ORG': [(13, 13)], 'PER': [(15, 16)]}\n",
            "Entity Level Mean F1 score is : 0.75\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "pred_token_labels = [\"ORG\", \"O\", \"PER\", \"PER\", \"O\", \"LOC\", \"O\", \"O\", \"O\", \"O\", \"MISC\", \"O\", \"O\", \"O\", \"O\", \"LOC\"]\n",
        "true_token_labels = [\"ORG\", \"O\", \"PER\", \"PER\", \"O\", \"LOC\", \"O\", \"O\", \"O\", \"O\", \"MISC\", \"MISC\", \"O\", \"O\", \"O\", \"LOC\"]\n",
        "token_indices = [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]\n",
        "\n",
        "y_pred_dict = format_output_labels(pred_token_labels, token_indices)\n",
        "print(\"y_pred_dict is : \" + str(y_pred_dict))\n",
        "y_true_dict = format_output_labels(true_token_labels, token_indices)\n",
        "print(\"y_true_dict is : \" + str(y_true_dict))\n",
        "\n",
        "print(\"Entity Level Mean F1 score is : \" + str(mean_f1(y_pred_dict, y_true_dict)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2TOwAs9JIesA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The F1 score is:  0.674774266620459\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# PREPROCESSING our data for our ML model ------------------------------#\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "\n",
        "def word2vec(word):\n",
        "    return nlp.vocab[word].vector\n",
        "\n",
        "\n",
        "def process_data(data, categories, pos_categories, train = True):\n",
        "    processed = []\n",
        "    for i in range(len(data)):\n",
        "        words = data['text'].iloc[i]\n",
        "        pos = data['POS'].iloc[i]\n",
        "        ner = []\n",
        "        ind = data['index'].iloc[i]\n",
        "        if(train):\n",
        "            ner = data['NER'].iloc[i]\n",
        "\n",
        "        for j in range(len(words)):\n",
        "            row = {\n",
        "                'word': words[j],\n",
        "                'pos': pos[j],\n",
        "                'vec' : word2vec(words[j]),\n",
        "                'ind' : ind[j]\n",
        "            }\n",
        "\n",
        "            if(train):\n",
        "                row['y'] = ner[j]\n",
        "\n",
        "            processed.append(row)\n",
        "    \n",
        "    processed = pd.DataFrame(processed)\n",
        "\n",
        "    # One hot encoding\n",
        "    if(train):\n",
        "        # set every row to an array of zeros\n",
        "        encoded = []\n",
        "        poss = []\n",
        "\n",
        "        for row in range(len(processed)):\n",
        "            arr = np.zeros(len(categories))\n",
        "            arr[categories.tolist().index(processed['y'].iloc[row])] = 1\n",
        "\n",
        "            # encode the pos column\n",
        "            pos_arr = np.zeros(len(pos_categories))\n",
        "            pos_arr[pos_categories.tolist().index(processed['pos'].iloc[row])] = 1\n",
        "\n",
        "            encoded.append(arr)\n",
        "            poss.append(pos_arr)\n",
        "    \n",
        "        processed['y_encoded'] = encoded\n",
        "        processed['pos_encoded'] = poss\n",
        "\n",
        "        processed['x'] = processed.apply(lambda x : np.concatenate((x['vec'], x['pos_encoded'])), axis=1)\n",
        "    else:\n",
        "        poss = []\n",
        "        for row in range(len(processed)):\n",
        "            pos_arr = np.zeros(len(pos_categories))\n",
        "            pos_arr[pos_categories.tolist().index(processed['pos'].iloc[row])] = 1\n",
        "\n",
        "            poss.append(pos_arr)\n",
        "\n",
        "        processed['pos_encoded'] = poss\n",
        "        processed['x'] = processed.apply(lambda x : np.concatenate((x['vec'], x['pos_encoded'])), axis=1)\n",
        "    \n",
        "    return processed\n",
        "\n",
        "concat = []\n",
        "concat2 = []\n",
        "for i in range(len(train)):\n",
        "    concat = concat + train['NER'].iloc[i]\n",
        "    concat2 = concat2 + train['POS'].iloc[i]\n",
        "\n",
        "categories = np.unique(concat)\n",
        "pos_categories = np.unique(concat2)\n",
        "\n",
        "train, validation = train_test_split(train, test_size=0.2, random_state=42)\n",
        "train_processed = process_data(train, categories, pos_categories)\n",
        "validation_processed = process_data(validation, categories, pos_categories)\n",
        "test_processed = process_data(test, categories, pos_categories, train=False)\n",
        "\n",
        "print(\"Val length \")\n",
        "\n",
        "\n",
        "TRAIN_NOW = False\n",
        "if(TRAIN_NOW):\n",
        "    # TRAINING our model ---------------------------------------------------#\n",
        "    import tensorflow as tf\n",
        "\n",
        "    # Make a nueral network with (345 input nodes, 5 hidden layers with dropout, 5 output nodes)\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Dense(345, input_shape=(345,), activation='tanh'),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(200, activation='tanh'),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(20, activation='tanh'),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(5, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                    loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy'])\n",
        "\n",
        "    # Train the model and save the best model\n",
        "    from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "    checkpoint_name = \"modelCheckpoints/Weights-matchmodel2-{epoch:03d}--{val_loss:.5f}.hdf5\"\n",
        "    checkpoint = ModelCheckpoint(checkpoint_name, monitor=\"val_loss\", verbose=1, save_best_only=True, mode=\"auto\")\n",
        "    callbacks_list = [checkpoint]\n",
        "\n",
        "    model.fit(np.array(train_processed['x'].tolist()), np.array(train_processed['y_encoded'].tolist()), epochs=60, batch_size=32, callbacks=callbacks_list, validation_data=(np.array(validation_processed['x'].tolist()), np.array(validation_processed['y_encoded'].tolist())))\n",
        "    # ------------------------------------------------------------------#\n",
        "\n",
        "\n",
        "# TESTING our model --------------------------------------------------#\n",
        "model = tf.keras.models.load_model('modelCheckpoints/Weights-matchmodel2-059--0.15338.hdf5')\n",
        "# Make predictions\n",
        "predictions = model.predict(np.array(validation_processed['x'].tolist()), verbose=0)\n",
        "\n",
        "# Get the index of the highest value in each row\n",
        "predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "true_token_labels = validation_processed['y'].tolist()\n",
        "pred_token_labels = [categories[pred] for pred in predictions]\n",
        "token_indices = validation_processed['ind'].tolist()\n",
        "\n",
        "y_pred_dict = format_output_labels(pred_token_labels, token_indices)\n",
        "y_true_dict = format_output_labels(true_token_labels, token_indices)\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1_score = mean_f1(y_true_dict, y_pred_dict)\n",
        "\n",
        "print(\"The F1 score is: \", f1_score)\n",
        "# ------------------------------------------------------------------#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "893l9j77ETFM"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "def create_submission(output_filepath, token_labels, token_inds):\n",
        "\n",
        "    label_dict = format_output_labels(token_labels, token_inds)\n",
        "    with open(output_filepath, mode='w') as csv_file:\n",
        "        fieldnames = ['Id', 'Predicted']\n",
        "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for key in label_dict:\n",
        "            p_string = \" \".join([str(start)+\"-\"+str(end) for start,end in label_dict[key]])\n",
        "            writer.writerow({'Id': key, 'Predicted': p_string})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "puOjjyyRzWDD"
      },
      "outputs": [],
      "source": [
        "\n",
        "import random\n",
        "random.seed(43)\n",
        "\n",
        "test_pred_labels = []\n",
        "test_pred_inds = []\n",
        "\n",
        "predictions = model.predict(np.array(test_processed['x'].tolist()), verbose=0)\n",
        "predictions = np.argmax(predictions, axis=1)\n",
        "predictions = [categories[pred] for pred in predictions]\n",
        "\n",
        "test_pred_labels = predictions\n",
        "test_pred_inds = test_processed['ind'].tolist()\n",
        "\n",
        "# generate the file with predictions (the predicted_random.csv entry on kaggle)\n",
        "create_submission(path + \"predicted.csv\", test_pred_labels, test_pred_inds)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "90d02fb60df52d0e9b9e3fdd36b86ce705e65dc7716bde2cb90fe6c36a5e0362"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
